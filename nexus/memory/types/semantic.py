"""è¯­ä¹‰è®°å¿†å®ç°

ç»“åˆå‘é‡æ£€ç´¢å’ŒçŸ¥è¯†å›¾è°±çš„æ··åˆè¯­ä¹‰è®°å¿†ï¼Œä½¿ç”¨ï¼š
- HuggingFace ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ–‡æœ¬åµŒå…¥
- å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢è¿›è¡Œå¿«é€Ÿåˆç­›
- çŸ¥è¯†å›¾è°±è¿›è¡Œå®ä½“å…³ç³»æ¨ç†
- æ··åˆæ£€ç´¢ç­–ç•¥ä¼˜åŒ–ç»“æœè´¨é‡
"""
import traceback
from typing import List, Dict, Any, Optional, Set, Tuple
from datetime import datetime, timedelta
import json
import logging
import math
import numpy as np

from ..base import BaseMemory, MemoryItem, MemoryConfig
from ..embedding import get_text_embedder, get_dimension


# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class Entity:
    """å®ä½“ç±»"""
    def __init__(
            self,
            entity_id: str,
            name: str,
            entity_type: str,
            description: str,
            properties: Dict[str, Any] = None
    ):
        self.entity_id = entity_id
        self.name = name
        self.entity_type = entity_type  # PERSON, ORG, PRODUCT, SKILL, CONCEPTç­‰
        self.description = description
        self.properties = properties or {}
        self.created_at = datetime.now()
        self.updated_at = datetime.now()
        self.frequency = 1      # å‡ºç°é¢‘ç‡

    def to_dict(self) -> Dict[str, Any]:
        return {
            "entity_id": self.entity_id,
            "name": self.name,
            "entity_type": self.entity_type,
            "description": self.description,
            "properties": self.properties,
            "frequency": self.frequency
        }

class Relation:
    """å…³ç³»ç±»"""
    def __init__(
            self,
            from_entity: str,
            to_entity: str,
            relation_type: str,
            strength: float = 1.0,
            evidence: str = "",
            properties: Dict[str, Any] = None
    ):
        self.from_entity = from_entity
        self.to_entity = to_entity
        self.relation_type = relation_type
        self.strength = strength
        self.evidence = evidence    # æ”¯æŒè¯¥å…³ç³»çš„åŸæ–‡æœ¬
        self.properties = properties or {}
        self.created_at = datetime.now()
        self.frequency = 1  # å…³ç³»å‡ºç°é¢‘ç‡

    def to_dict(self) -> Dict[str, Any]:
        return {
            "from_entity": self.from_entity,
            "to_entity": self.to_entity,
            "relation_type": self.relation_type,
            "strength": self.strength,
            "evidence": self.evidence,
            "properties": self.properties,
            "frequency": self.frequency
        }

class SemanticMemory(BaseMemory):
    """
    å¢å¼ºè¯­ä¹‰è®°å¿†å®ç°

    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨HuggingFaceä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ–‡æœ¬åµŒå…¥
    - å‘é‡æ£€ç´¢è¿›è¡Œå¿«é€Ÿç›¸ä¼¼åº¦åŒ¹é…
    - çŸ¥è¯†å›¾è°±å­˜å‚¨å®ä½“å’Œå…³ç³»
    - æ··åˆæ£€ç´¢ç­–ç•¥ï¼šå‘é‡+å›¾+è¯­ä¹‰æ¨ç†
    """
    def __init__(self, config: MemoryConfig, storage_backend=None):
        super().__init__(config, storage_backend)

        # åµŒå…¥æ¨¡å‹ï¼ˆç»Ÿä¸€æä¾›ï¼‰
        self.embedding_model = None
        self._init_embedding_model()

        # ä¸“ä¸šæ•°æ®åº“å­˜å‚¨
        self.vector_store = None
        self.graph_store = None
        self._init_databases()

        # å®ä½“å’Œå…³ç³»å­˜å‚¨ï¼ˆç”¨äºå¿«é€Ÿè®¿é—®ï¼‰
        self.entities: Dict[str, Entity] = {}
        self.relations: List[Relation] = []

        # å®ä½“è¯†åˆ«å™¨
        self.nlp = None
        self._init_nlp()

        # è®°å¿†å­˜å‚¨
        self.semantic_memories: List[MemoryItem] = []
        self.memory_embeddings: Dict[str, np.ndarray] = {}

        logger.info("å¢å¼ºè¯­ä¹‰è®°å¿†åˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨Qdrant+Neo4jä¸“ä¸šæ•°æ®åº“ï¼‰")


    def _init_embedding_model(self):
        """åˆå§‹åŒ–ç»Ÿä¸€åµŒå…¥æ¨¡å‹ï¼ˆç”± embedding_provider ç®¡ç†ï¼‰ã€‚"""
        try:
            self.embedding_model = get_text_embedder()
            # è½»é‡å¥åº·æ£€æŸ¥ä¸æ—¥å¿—
            try:
                test_vec = self.embedding_model.encode("health_check")
                dim = getattr(self.embedding_model, "dimension", len(test_vec))
                logger.info(f"âœ… åµŒå…¥æ¨¡å‹å°±ç»ªï¼Œç»´åº¦: {dim}")
            except Exception:
                logger.info("âœ… åµŒå…¥æ¨¡å‹å°±ç»ª")
        except Exception as e:
            logger.error(f"âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _init_databases(self):
        """åˆå§‹åŒ–ä¸“ä¸šæ•°æ®åº“å­˜å‚¨"""
        try:
            from ...core.database_config import get_database_config
            # è·å–æ•°æ®åº“é…ç½®
            db_config = get_database_config()

            # åˆå§‹åŒ–Qdrantå‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨è¿æ¥ç®¡ç†å™¨é¿å…é‡å¤è¿æ¥ï¼‰
            from ..storage.qdrant_store import QdrantConnectionManager
            qdrant_config = db_config.get_qdrant_config() or {}
            qdrant_config["vector_size"] = get_dimension()
            self.vector_store = QdrantConnectionManager.get_instance(**qdrant_config)
            logger.info("âœ… Qdrantå‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

            # åˆå§‹åŒ–Neo4jå›¾æ•°æ®åº“
            from ..storage.neo4j_store import Neo4jGraphStore
            neo4j_config = db_config.get_neo4j_config()
            self.graph_store = Neo4jGraphStore(**neo4j_config)
            logger.info("âœ… Neo4jå›¾æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

            # éªŒè¯è¿æ¥
            vector_health = self.vector_store.health_check()
            graph_health = self.graph_store.health_check()

            if not vector_health:
                logger.warning("âš ï¸ Qdrantè¿æ¥å¼‚å¸¸ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—é™")
            if not graph_health:
                logger.warning("âš ï¸ Neo4jè¿æ¥å¼‚å¸¸ï¼Œå›¾æœç´¢åŠŸèƒ½å¯èƒ½å—é™")

            logger.info(
                f"ğŸ¥ æ•°æ®åº“å¥åº·çŠ¶æ€: Qdrant={'âœ…' if vector_health else 'âŒ'}, Neo4j={'âœ…' if graph_health else 'âŒ'}")

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.info("ğŸ’¡ è¯·æ£€æŸ¥æ•°æ®åº“é…ç½®å’Œç½‘ç»œè¿æ¥")
            logger.info("ğŸ’¡ å‚è€ƒ DATABASE_SETUP_GUIDE.md è¿›è¡Œé…ç½®")
            raise

    def _init_nlp(self):
        """åˆå§‹åŒ–NLPå¤„ç†å™¨ - æ™ºèƒ½å¤šè¯­è¨€æ”¯æŒ"""
        try:
            import spacy
            self.nlp_models = {}

            # å°è¯•åŠ è½½å¤šè¯­è¨€æ¨¡å‹
            models_to_try = [
                ("zh_core_web_sm", "ä¸­æ–‡"),
                ("en_core_web_sm", "è‹±æ–‡")
            ]

            loaded_models = []
            for model_name, lang_name in models_to_try:
                try:
                    nlp = spacy.load(model_name)
                    self.nlp_models[model_name] = nlp
                    loaded_models.append(lang_name)
                    logger.info(f"âœ… åŠ è½½{lang_name}spaCyæ¨¡å‹: {model_name}")
                except OSError:
                    logger.warning(f"âš ï¸ {lang_name}spaCyæ¨¡å‹ä¸å¯ç”¨: {model_name}")

            # è®¾ç½®ä¸»è¦NLPå¤„ç†å™¨
            if "zh_core_web_sm" in self.nlp_models:
                self.nlp = self.nlp_models["zh_core_web_sm"]
                logger.info("ğŸ¯ ä¸»è¦ä½¿ç”¨ä¸­æ–‡spaCyæ¨¡å‹")
            elif "en_core_web_sm" in self.nlp_models:
                self.nlp = self.nlp_models["en_core_web_sm"]
                logger.info("ğŸ¯ ä¸»è¦ä½¿ç”¨è‹±æ–‡spaCyæ¨¡å‹")
            else:
                self.nlp = None
                logger.warning("âš ï¸ æ— å¯ç”¨spaCyæ¨¡å‹ï¼Œå®ä½“æå–å°†å—é™")

            if loaded_models:
                logger.info(f"ğŸ“š å¯ç”¨è¯­è¨€æ¨¡å‹: {', '.join(loaded_models)}")

        except ImportError:
            logger.warning("âš ï¸ spaCyä¸å¯ç”¨ï¼Œå®ä½“æå–å°†å—é™")
            self.nlp = None
            self.nlp_models = {}

    def add(self, memory_item: MemoryItem) -> str:
        """æ·»åŠ è¯­ä¹‰è®°å¿†"""
        try:
            # 1. ç”Ÿæˆæ–‡æœ¬åµŒå…¥
            embedding = self.embedding_model.encode(memory_item.content)
            self.memory_embeddings[memory_item.id] = embedding

            # 2. æå–å®ä½“å’Œå…³ç³»
            entities = self._extract_entities(memory_item.content)
            relations = self._extract_relations(memory_item.content, entities)

            # 3. å­˜å‚¨åˆ°Neo4jæ•°æ®åº“
            for entity in entities:
                self._add_entity_to_graph(entity, memory_item)

            for relation in relations:
                self._add_relation_to_graph(relation, memory_item)

            # 4. å­˜å‚¨åˆ°Qdrantæ•°æ®åº“
            metadata = {
                "memory_id": memory_item.id,
                "user_id": memory_item.user_id,
                "content": memory_item.content,
                "memory_type": memory_item.memory_type,
                "timestamp": int(memory_item.timestamp.timestamp()),
                "importance": memory_item.importance,
                "entities": [e.entity_id for e in entities],
                "entity_count": len(entities),
                "relation_count": len(relations)
            }

            success = self.vector_store.add_vectors(
                vectors=[embedding.tolist()],
                metadata=[metadata],
                ids=[memory_item.id]
            )

            if not success:
                logger.warning("âš ï¸ å‘é‡å­˜å‚¨å¤±è´¥ï¼Œä½†è®°å¿†å·²æ·»åŠ åˆ°å›¾æ•°æ®åº“")

            # 5. æ·»åŠ å®ä½“ä¿¡æ¯åˆ°å…ƒæ•°æ®
            memory_item.metadata["entities"] = [e.entity_id for e in entities]
            memory_item.metadata["relations"] = [
                f"{r.from_entity}-{r.relation_type}-{r.to_entity}" for r in relations
            ]

            # 6. å­˜å‚¨è®°å¿†
            self.semantic_memories.append(memory_item)

            logger.info(f"âœ… æ·»åŠ è¯­ä¹‰è®°å¿†: {len(entities)}ä¸ªå®ä½“, {len(relations)}ä¸ªå…³ç³»")
            return memory_item.id
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ è¯­ä¹‰è®°å¿†å¤±è´¥: {e}")
            raise

    def _extract_entities(self, text: str) -> List[Entity]:
        """æ™ºèƒ½å¤šè¯­è¨€å®ä½“æå–"""
        entities = []

        # æ£€æµ‹æ–‡æœ¬è¯­è¨€
        lang = self._detect_language(text)

        # é€‰æ‹©åˆé€‚çš„spaCyæ¨¡å‹
        selected_nlp = None
        if lang == "zh" and "zh_core_web_sm" in self.nlp_models:
            selected_nlp = self.nlp_models["zh_core_web_sm"]
        elif lang == "en" and "en_core_web_sm" in self.nlp_models:
            selected_nlp = self.nlp_models["en_core_web_sm"]
        else:
            # ä½¿ç”¨é»˜è®¤æ¨¡å‹
            selected_nlp = self.nlp

        logger.debug(f"ğŸŒ æ£€æµ‹è¯­è¨€: {lang}, ä½¿ç”¨æ¨¡å‹: {selected_nlp.meta['name'] if selected_nlp else 'None'}")

        # ä½¿ç”¨spaCyè¿›è¡Œå®ä½“è¯†åˆ«å’Œè¯æ³•åˆ†æ
        if selected_nlp:
            try:
                doc = selected_nlp(text)
                logger.debug(f"ğŸ“ spaCyå¤„ç†æ–‡æœ¬: '{text}' -> {len(doc.ents)} ä¸ªå®ä½“")

                # å­˜å‚¨è¯æ³•åˆ†æç»“æœï¼Œä¾›Neo4jä½¿ç”¨
                self._store_linguistic_analysis(doc, text)

                if not doc.ents:
                    # å¦‚æœæ²¡æœ‰å®ä½“ï¼Œè®°å½•è¯¦ç»†çš„è¯å…ƒä¿¡æ¯
                    logger.debug("ğŸ” æœªæ‰¾åˆ°å®ä½“ï¼Œè¯å…ƒåˆ†æ:")
                    for token in doc[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªè¯å…ƒ
                        logger.debug(f"   '{token.text}' -> POS: {token.pos_}, TAG: {token.tag_}, ENT_IOB: {token.ent_iob_}")

                for ent in doc.ents:
                    entity = Entity(
                        entity_id=f"entity_{hash(ent.text)}",
                        name=ent.text,
                        entity_type=ent.label_,
                        description=f"ä»æ–‡æœ¬ä¸­è¯†åˆ«çš„{ent.label_}å®ä½“"
                    )
                    entities.append(entity)
                    # å®‰å…¨è·å–ç½®ä¿¡åº¦ä¿¡æ¯
                    confidence = "N/A"

                    try:
                        if hasattr(ent._, 'confidence'):
                            confidence = getattr(ent._, 'confidence', 'N/A')
                    except:
                        confidence = "N/A"

                    logger.debug(f"ğŸ·ï¸ spaCyè¯†åˆ«å®ä½“: '{ent.text}' -> {ent.label_} (ç½®ä¿¡åº¦: {confidence})")
            except Exception as e:
                logger.warning(f"âš ï¸ spaCyå®ä½“è¯†åˆ«å¤±è´¥: {e}")
                logger.debug(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        else:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„spaCyæ¨¡å‹è¿›è¡Œå®ä½“è¯†åˆ«")

        return entities

    def _extract_relations(self, text: str, entities: List[Entity]) -> List[Relation]:
        """æå–å…³ç³»"""
        relations = []
        # ä»…ä¿ç•™ç®€å•å…±ç°å…³ç³»ï¼Œä¸åšä»»ä½•æ­£åˆ™/å…³é”®è¯åŒ¹é…
        for i, entity1 in enumerate(entities):
            for entity2 in entities[i + 1:]:
                relations.append(Relation(
                    from_entity=entity1.entity_id,
                    to_entity=entity2.entity_id,
                    relation_type="CO_OCCURS",
                    strength=0.5,
                    evidence=text[:100]
                ))
        return relations

    def _detect_language(self, text: str) -> str:
        """ç®€å•çš„è¯­è¨€æ£€æµ‹"""
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹ï¼ˆæ— æ­£åˆ™ï¼Œé€å­—ç¬¦åˆ¤æ–­èŒƒå›´ï¼‰
        chinese_chars = sum(1 for ch in text if '\u4e00' <= ch <= '\u9fff')
        total_chars = len(text.replace(' ', ''))

        if total_chars == 0:
            return "en"

        chinese_ratio = chinese_chars / total_chars
        return "zh" if chinese_ratio > 0.3 else "en"

    def _store_linguistic_analysis(self, doc, text: str):
        """å­˜å‚¨spaCyè¯æ³•åˆ†æç»“æœåˆ°Neo4j"""
        if not self.graph_store:
            return

        try:
            # ä¸ºæ¯ä¸ªè¯å…ƒå»ºç«‹èŠ‚ç‚¹
            for token in doc:
                # è·³è¿‡æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
                if token.is_input or token .is_space:
                    continue

                token_id = f"token_{hash(token.text + token.pos_)}"

                # æ·»åŠ è¯å…ƒèŠ‚ç‚¹åˆ°Neo4j
                self.graph_store.add_entity(
                    entity_id=token_id,
                    name=token.text,
                    entity_type="TOKEN",
                    properties={
                        "pos": token.pos_,  # è¯æ€§ï¼ˆNOUN, VERBç­‰ï¼‰
                        "tag": token.tag_,  # ç»†ç²’åº¦æ ‡ç­¾
                        "lemma": token.lemma_,  # è¯å…ƒåŸå½¢
                        "is_alpha": token.is_alpha,
                        "is_stop": token.is_stop,
                        "source_text": text[:50],  # æ¥æºæ–‡æœ¬ç‰‡æ®µ
                        "language": self._detect_language(text)
                    }
                )

                # å¦‚æœæ˜¯åè¯ï¼Œå¯èƒ½å­˜åœ¨æ½œåœ¨çš„æ¦‚å¿µ
                if token.pos_ in ["NOUN", "PROPN"]:
                    concept_id = f"concept_{hash(token.text)}"
                    self.graph_store.add_entity(
                        entity_id=concept_id,
                        name=token.text,
                        entity_type="CONCEPT",
                        properties={
                            "category": token.pos_,
                            "frequency": 1,  # å¯ä»¥åç»­ç´¯è®¡
                            "source_text": text[:50]
                        }
                    )

                    # å»ºç«‹è¯å…ƒåˆ°æ¦‚å¿µçš„å…³ç³»
                    self.graph_store.add_relationship(
                        from_entity_id=token_id,
                        to_entity_id=concept_id,
                        relationship_type="REPRESENTS",
                        properties={"confidence": 1.0}
                    )


            # å»ºç«‹è¯å…ƒä¹‹é—´çš„ä¾å­˜å…³ç³»
            for token in doc:
                if token.is_input or token.is_space or token.head == token:
                    continue

                from_id = f"token_{hash(token.text + token.pos_)}"
                to_id = f"token_{hash(token.head.text + token.head.pos_)}"

                # Neo4jä¸å…è®¸å…³ç³»ç±»å‹åŒ…å«å†’å·ï¼Œéœ€è¦æ¸…ç†
                relation_type = token.dep_.upper().replace(":", "_")

                self.graph_store.add_relationship(
                    from_entity_id=from_id,
                    to_entity_id=to_id,
                    relationship_type=relation_type,    # æ¸…ç†åçš„ä¾å­˜å…³ç³»ç±»å‹
                    properties={
                        "dependency": token.dep_,   # ä¿ç•™åŸå§‹ä¾å­˜å…³ç³»
                        "source_text": text[:50]
                    }
                )

            logger.debug(f"ğŸ”— å·²å°†è¯æ³•åˆ†æç»“æœå­˜å‚¨åˆ°Neo4j: {len([t for t in doc if not t.is_punct and not t.is_space])} ä¸ªè¯å…ƒ")

        except Exception as e:
            logger.warning(f"âš ï¸ å­˜å‚¨è¯æ³•åˆ†æå¤±è´¥: {e}")

    def _add_entity_to_graph(self, entity: Entity, memory_item: MemoryItem):
        """æ·»åŠ å®ä½“åˆ°Neo4jå›¾æ•°æ®åº“"""
        try:
            # å‡†å¤‡å®ä½“å±æ€§
            properties = {
                "name": entity.name,
                "description": entity.description,
                "frequency": entity.frequency,
                "memory_id": memory_item.id,
                "user_id": memory_item.user_id,
                "importance": memory_item.importance,
                **entity.properties
            }

            # æ·»åŠ åˆ°Neo4j
            success = self.graph_store.add_entity(
                entity_id=entity.entity_id,
                name=entity.name,
                entity_type=entity.entity_type,
                properties=properties
            )

            if success:
                # åŒæ—¶æ›´æ–°æœ¬åœ°ç¼“å­˜
                if entity.entity_id in self.entities:
                    self.entities[entity.entity_id].frequency += 1
                    self.entities[entity.entity_id].updated_at = datetime.now()
                else:
                    self.entities[entity.entity_id] = entity

            return success

        except Exception as e:
            logger.error(f"âŒ æ·»åŠ å®ä½“åˆ°å›¾æ•°æ®åº“å¤±è´¥: {e}")
            return False

    def _add_relation_to_graph(self, relation: Relation, memory_item: MemoryItem):
        """æ·»åŠ å…³ç³»åˆ°Neo4jå›¾æ•°æ®åº“"""
        try:
            # å‡†å¤‡å…³ç³»å±æ€§
            properties = {
                "strength": relation.strength,
                "memory_id": memory_item.id,
                "user_id": memory_item.user_id,
                "importance": memory_item.importance,
                "evidence": relation.evidence
            }

            # æ·»åŠ åˆ°Neo4j
            success = self.graph_store.add_relationship(
                from_entity_id=relation.from_entity,
                to_entity_id=relation.to_entity,
                relationship_type=relation.relation_type,
                properties=properties
            )

            if success:
                # åŒæ—¶æ›´æ–°æœ¬åœ°ç¼“å­˜
                self.relations.append(relation)

            return success

        except Exception as e:
            logger.error(f"âŒ æ·»åŠ å…³ç³»åˆ°å›¾æ•°æ®åº“å¤±è´¥: {e}")
            return False

